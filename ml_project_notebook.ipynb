{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c539ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class co2TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, co2_file):\n",
    "        self.df = pd.read_csv(co2_file)\n",
    "        self.features = self.df.columns \n",
    "        self.df.insert(len(self.features), \"target\", pd.Series(self.df['co2'][1:].values))\n",
    "        self.df = self.df.dropna()\n",
    "        self.size = len(self.df)\n",
    "        self.data = torch.tensor(np.array(self.df), dtype=torch.float).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return (co2_values, co2_next_year)\n",
    "        return self.data[idx, :, 1:2], self.data[idx, -1, 2:]\n",
    "    \n",
    "    def create_window_dataset(self, window_size, starting_idx=0):\n",
    "        self.data = torch.tensor( np.array([ self.data[starting_idx+idx : starting_idx + idx+window_size].numpy() for idx in range(self.size) if (starting_idx + idx + window_size) <= self.size ]))\n",
    "    \n",
    "    def scale_data(self):\n",
    "        self.scaler = self.scaler.fit(self.data[:,-1].reshape(-1,1))\n",
    "        self.data[:,-1] = torch.tensor(self.scaler.transform(self.data[:,-1].reshape(-1,1))).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb45bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfolds(k, dataset):\n",
    "    size = len(dataset)\n",
    "    size_fold = [ size // k for x in range(k) ]\n",
    "    if sum(size_fold) != size:\n",
    "        size_fold[-1] += size - (size_fold[0]*k)\n",
    "    kfolds = torch.utils.data.random_split(dataset, [x for x in size_fold])\n",
    "    return kfolds\n",
    "\n",
    "def stop_criteria(losses, thr):\n",
    "    if len(losses) < 2:\n",
    "        return (False,)\n",
    "    if statistics.stdev(losses) < thr:\n",
    "        return (True, statistics.stdev(losses))\n",
    "    return (False, statistics.stdev(losses))\n",
    "\n",
    "def combinations_parameters(parameters):\n",
    "    param = list(parameters.keys())\n",
    "    res = []\n",
    "    idx = [0 for x in range(len(param))]\n",
    "    update_idx = len(idx)-1\n",
    "    loop = True\n",
    "    while True:\n",
    "        if update_idx == -1:\n",
    "            break\n",
    "        res.append([ parameters[x][idx[param.index(x)] ] for x in param])\n",
    "        update_idx = len(idx) -1\n",
    "        idx[update_idx] += 1\n",
    "        while idx[update_idx] == len(parameters[param[update_idx]]):\n",
    "            idx[update_idx] = 0\n",
    "            update_idx -= 1\n",
    "            if update_idx == -1:\n",
    "                break\n",
    "            idx[update_idx] +=1\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd94e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldCrossValidation(dataset, k, model_name, parameters, stop_size_std):\n",
    "    \n",
    "    # create a list with the k folds\n",
    "    kfolds = Kfolds(k, dataset)\n",
    "    validation_losses = []\n",
    "    print(f\"Model: {model_name}\\nParameters: {parameters}\")\n",
    "    # k-fold cross validation\n",
    "    for x in range(k):\n",
    "    \n",
    "        # model setup\n",
    "        model_name = model_name.lower()\n",
    "        if model_name == \"rnn\":\n",
    "            model = RNN(parameters[\"input_size\"], parameters[\"hidden_size\"], parameters[\"num_layers\"], parameters[\"output_size\"]).to(device)\n",
    "        elif model_name == \"lstm\":\n",
    "            model = LSTM(parameters[\"input_size\"], parameters[\"hidden_size\"], parameters[\"num_layers\"], parameters[\"output_size\"]).to(device)\n",
    "        elif model_name == \"gru\":\n",
    "            model = GRU(parameters[\"input_size\"], parameters[\"hidden_size\"], parameters[\"num_layers\"], parameters[\"output_size\"]).to(device)\n",
    "        \n",
    "        # loss and optimizer selection\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"learning_rate\"])\n",
    "    \n",
    "        # kfolds split in test and validation sets\n",
    "        print(f\"Fold {x+1}\")\n",
    "        train_set_k = torch.utils.data.ConcatDataset([kfolds[c] for c in range(k) if c != x])\n",
    "        train_dataset_loader = DataLoader(train_set_k, batch_size=parameters[\"batch_size\"], shuffle=True)\n",
    "        validation_dataset_loader = DataLoader(kfolds[x], batch_size=parameters[\"batch_size\"], shuffle=True)\n",
    "    \n",
    "        # train until convergence\n",
    "        losses = []\n",
    "        epoch = 1\n",
    "        while True:\n",
    "        \n",
    "            loss_batch = []\n",
    "            for i, (features, labels) in enumerate(train_dataset_loader):\n",
    "        \n",
    "                # Forward pass\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "        \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_batch.append(loss.item())\n",
    "                \n",
    "            # check stopping criterion every epoch\n",
    "            mean_loss_batch = statistics.mean(loss_batch)\n",
    "            losses.append(mean_loss_batch)\n",
    "            stop = stop_criteria(losses, 10*parameters[\"learning_rate\"])\n",
    "            if stop[0] or epoch==7000:\n",
    "                break\n",
    "            if len(losses) == stop_size_std[0]:\n",
    "                losses = losses[1:]\n",
    "            if epoch%10 == 0:\n",
    "                print(f\"Epoch {epoch}\\nLoss {mean_loss_batch}\")\n",
    "                print(f\"stdev: {stop[1]}\\n\")\n",
    "            epoch+=1\n",
    "    \n",
    "        # perform validation\n",
    "        with torch.no_grad():\n",
    "            validation_batch_losses = []\n",
    "            for features, labels in validation_dataset_loader:\n",
    "                outputs = model(features)\n",
    "        \n",
    "                loss = criterion(outputs,labels)\n",
    "                validation_batch_losses.append(loss.item())\n",
    "        \n",
    "            validation_batch_loss_mean = statistics.mean(validation_batch_losses)\n",
    "            print(f\"Validation Loss: {validation_batch_loss_mean}\")\n",
    "            validation_losses.append(validation_batch_loss_mean)\n",
    "    return statistics.mean(validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d556818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HyperParametersTuning(dataset, k, model, hyperparam, stop_criteria):\n",
    "    combinations = combinations_parameters(hyperparam)\n",
    "    res_tuning = {}\n",
    "    for comb in combinations:\n",
    "        temp_hyperparam = {}\n",
    "        i=0\n",
    "        for par in hyperparam:\n",
    "            temp_hyperparam[par] = comb[i]\n",
    "            i+=1\n",
    "        mean = kFoldCrossValidation(dataset, k, model, temp_hyperparam, stop_criteria)\n",
    "        res_tuning[mean] = comb\n",
    "    best = min(list(res_tuning.keys()))\n",
    "    return (best, res_tuning[best])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80650fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining RNN model\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.rand(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # take only the last prediction of the sequence\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ace6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.rand(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.rand(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0,c0))\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        \n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, defining sequence length\n",
    "sequence_length = 10\n",
    "co2_dataset = co2TimeSeriesDataset(\"./datasets/co2 dataset/1800_co2.csv\")\n",
    "co2_dataset.create_window_dataset(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbddc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stopping criteria\n",
    "size_queue = 10\n",
    "std_dev = 0.09\n",
    "\n",
    "# k for kfolds\n",
    "k = 5\n",
    "\n",
    "# Hyperparameters for rnn\n",
    "hyperparam = {\"input_size\" : [1],\n",
    "              \"output_size\" : [1],\n",
    "              \"hidden_size\" : [2, 5, 10],\n",
    "              \"num_layers\" : [1, 2],\n",
    "              \"batch_size\" : [10, 20, 40],\n",
    "              \"learning_rate\" : [0.001, 0.01, 0.1]\n",
    "             }\n",
    "HyperParametersTuning(co2_dataset, k, \"rnn\", hyperparam, (size_queue, std_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a446680",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2408e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stopping criteria\n",
    "size_queue = 30\n",
    "std_dev = 0.1\n",
    "\n",
    "# k for kfolds\n",
    "k = 5\n",
    "\n",
    "# Hyperparameters for rnn\n",
    "hyperparam = {\"input_size\" : [1],\n",
    "              \"output_size\" : [1],\n",
    "              \"hidden_size\" : [10],#[30, 70, 100],\n",
    "              \"num_layers\" : [1],\n",
    "              \"batch_size\" : [5],\n",
    "              \"learning_rate\" : [0.001]#[0.001, 0.01]\n",
    "             }\n",
    "HyperParametersTuning(co2_dataset, k, \"lstm\", hyperparam, (size_queue, std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping criteria\n",
    "size_queue = 10\n",
    "std_dev = 0.09\n",
    "\n",
    "# k for kfolds\n",
    "k = 5\n",
    "\n",
    "# Hyperparameters for rnn\n",
    "hyperparam = {\"input_size\" : [1],\n",
    "              \"output_size\" : [1],\n",
    "              \"hidden_size\" : [5, 10, 20, 30, 40],\n",
    "              \"num_layers\" : [1],\n",
    "              \"batch_size\" : [10, 20, 40],\n",
    "              \"learning_rate\" : [0.001, 0.01, 0.1]\n",
    "             }\n",
    "HyperParametersTuning(co2_dataset, k, \"lstm\", hyperparam, (size_queue, std_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58b724",
   "metadata": {},
   "source": [
    "# Land-ocean temperature anomaly prediction from co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68422336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyTemperaturesTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.df = pd.read_csv(file)\n",
    "        self.features = self.df.columns \n",
    "        self.df = self.df.dropna()\n",
    "        self.size = len(self.df)\n",
    "        self.data = torch.tensor(np.array(self.df), dtype=torch.float).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, :, 1:2], self.data[idx, -1, 2:]\n",
    "    \n",
    "    def create_window_dataset(self, window_size, starting_idx=0):\n",
    "        self.data = torch.tensor( np.array([ self.data[starting_idx+idx : starting_idx + idx+window_size].numpy() for idx in range(self.size) if (starting_idx + idx + window_size) <= self.size ]))\n",
    "    \n",
    "    def scale_data(self):\n",
    "        self.scaler = self.scaler.fit(self.data[:,-1].reshape(-1,1))\n",
    "        self.data[:,-1] = torch.tensor(self.scaler.transform(self.data[:,-1].reshape(-1,1))).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b389ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, defining sequence length\n",
    "sequence_length = 10\n",
    "anomaly_temperatures_dataset = AnomalyTemperaturesTimeSeriesDataset(\"./datasets/land-ocean-temperatures/anomaly_temperatures.csv\")\n",
    "anomaly_temperatures_dataset.create_window_dataset(sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ac5ba",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b484b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stopping criteria\n",
    "size_queue = 10\n",
    "std_dev = 0.09\n",
    "\n",
    "# k for kfolds\n",
    "k = 5\n",
    "\n",
    "# Hyperparameters for rnn\n",
    "hyperparam = {\"input_size\" : [1],\n",
    "              \"output_size\" : [2],\n",
    "              \"hidden_size\" : [2, 5, 10],\n",
    "              \"num_layers\" : [1, 2],\n",
    "              \"batch_size\" : [20],\n",
    "              \"learning_rate\" : [0.001, 0.01, 0.1]\n",
    "             }\n",
    "HyperParametersTuning(anomaly_temperatures_dataset, k, \"rnn\", hyperparam, (size_queue, std_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998c8cb",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d6213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping criteria\n",
    "size_queue = 10\n",
    "std_dev = 0.09\n",
    "\n",
    "# k for kfolds\n",
    "k = 5\n",
    "\n",
    "# Hyperparameters for lstm\n",
    "hyperparam = {\"input_size\" : [1],\n",
    "              \"output_size\" : [2],\n",
    "              \"hidden_size\" : [5, 10, 20, 30, 40],\n",
    "              \"num_layers\" : [1],\n",
    "              \"batch_size\" : [10, 20, 40],\n",
    "              \"learning_rate\" : [0.001, 0.01, 0.1]\n",
    "             }\n",
    "HyperParametersTuning(anomaly_temperatures_dataset, k, \"lstm\", hyperparam, (size_queue, std_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5356d",
   "metadata": {},
   "source": [
    "# Mortality, sea level and wildfires prediction from anomaly temperatures of land and oceans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0484fc1",
   "metadata": {},
   "source": [
    "## Mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MortalityTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.df = pd.read_csv(file)\n",
    "        self.features = self.df.columns \n",
    "        self.df.insert(loc=3, column=\"mortality_feature\", value=self.df['mortality_x_100k'].shift(1))\n",
    "        self.df = self.df.dropna()\n",
    "        \n",
    "        self.size = len(self.df)\n",
    "        self.data = torch.tensor(np.array(self.df), dtype=torch.float).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, :, 1:4], self.data[idx, -1, 4:]\n",
    "    \n",
    "    def create_window_dataset(self, window_size, starting_idx=0):\n",
    "        self.data = torch.tensor( np.array([ self.data[starting_idx+idx : starting_idx + idx+window_size].numpy() for idx in range(self.size) if (starting_idx + idx + window_size) <= self.size ]))\n",
    "    \n",
    "    def scale_data(self):\n",
    "        self.scaler = self.scaler.fit(self.data[:,-1].reshape(-1,1))\n",
    "        self.data[:,-1] = torch.tensor(self.scaler.transform(self.data[:,-1].reshape(-1,1))).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de201803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, defining sequence length\n",
    "sequence_length = 10\n",
    "mortality_usa_dataset = MortalityTimeSeriesDataset(\"./datasets/mortality dataset/temp_death_usa.csv\")\n",
    "mortality_usa_dataset.create_window_dataset(sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b389865",
   "metadata": {},
   "source": [
    "FAILED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0061787c",
   "metadata": {},
   "source": [
    "## Sea Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeaLevelTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.df = pd.read_csv(file)\n",
    "        self.features = self.df.columns \n",
    "        self.df.insert(loc=3, column=\"GMSL_feature\", value=self.df['GMSL'].shift(1))\n",
    "        self.df = self.df.dropna()\n",
    "        \n",
    "        self.size = len(self.df)\n",
    "        self.data = torch.tensor(np.array(self.df), dtype=torch.float).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, :, 1:4], self.data[idx, -1, 4:]\n",
    "    \n",
    "    def create_window_dataset(self, window_size, starting_idx=0):\n",
    "        self.data = torch.tensor( np.array([ self.data[starting_idx+idx : starting_idx + idx+window_size].numpy() for idx in range(self.size) if (starting_idx + idx + window_size) <= self.size ]))\n",
    "    \n",
    "    def scale_data(self):\n",
    "        self.scaler = self.scaler.fit(self.data[:,-1].reshape(-1,1))\n",
    "        self.data[:,-1] = torch.tensor(self.scaler.transform(self.data[:,-1].reshape(-1,1))).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, defining sequence length\n",
    "sequence_length = 5\n",
    "sea_level_dataset = SeaLevelTimeSeriesDataset(\"./datasets/sea level dataset/clean_sea_levels.csv\")\n",
    "sea_level_dataset.create_window_dataset(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f105e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stopping criteria\n",
    "size_queue = 10\n",
    "std_dev = 0.09\n",
    "\n",
    "# k for kfolds\n",
    "k = 5\n",
    "\n",
    "# Hyperparameters for rnn\n",
    "hyperparam = {\"input_size\" : [3],\n",
    "              \"output_size\" : [1],\n",
    "              \"hidden_size\" : [2, 5, 10, 20],\n",
    "              \"num_layers\" : [1, 2],\n",
    "              \"batch_size\" : [10],\n",
    "              \"learning_rate\" : [0.001, 0.01, 0.1]\n",
    "             }\n",
    "HyperParametersTuning(sea_level_dataset, k, \"rnn\", hyperparam, (size_queue, std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762effb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stopping criteria\n",
    "size_queue = 10\n",
    "std_dev = 0.09\n",
    "\n",
    "# k for kfolds\n",
    "k = 5\n",
    "\n",
    "# Hyperparameters for rnn\n",
    "hyperparam = {\"input_size\" : [3],\n",
    "              \"output_size\" : [1],\n",
    "              \"hidden_size\" : [40, 70],\n",
    "              \"num_layers\" : [1],\n",
    "              \"batch_size\" : [10, 20, 40],\n",
    "              \"learning_rate\" : [0.001, 0.01, 0.1]\n",
    "             }\n",
    "HyperParametersTuning(sea_level_dataset, k, \"lstm\", hyperparam, (size_queue, std_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04bf65",
   "metadata": {},
   "source": [
    "# CO2 model prediction training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        train_size = 0.8\n",
    "        train_size = int(train_size * len(co2_dataset))\n",
    "        train_set, test_set = torch.utils.data.random_split(co2_dataset, [train_size, len(co2_dataset) - train_size])\n",
    "        train_set_loader = DataLoader(train_set, batch_size=40, shuffle=True)\n",
    "        test_set_loader = DataLoader(test_set, batch_size=40, shuffle=True)\n",
    "        co2_rnn_model = RNN(1, 10, 2, 1).to(device)\n",
    "        \n",
    "        # loss and optimizer selection\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(co2_rnn_model.parameters(), lr=0.01)\n",
    "        \n",
    "        # train until convergence\n",
    "        losses = []\n",
    "        history_loss = []\n",
    "        epoch = 1\n",
    "        while True:\n",
    "        \n",
    "            loss_batch = []\n",
    "            for i, (features, labels) in enumerate(train_set_loader):\n",
    "        \n",
    "                # Forward pass\n",
    "                outputs = co2_rnn_model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    " \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_batch.append(loss.item())\n",
    "                \n",
    "            # check stopping criterion every epoch\n",
    "            mean_loss_batch = statistics.mean(loss_batch)\n",
    "            losses.append(mean_loss_batch)\n",
    "            history_loss.append(mean_loss_batch)\n",
    "            stop = stop_criteria(losses, 0.01)\n",
    "            if stop[0] or epoch==3000:\n",
    "                break\n",
    "            if len(losses) == 10:\n",
    "                losses = losses[1:]\n",
    "            if epoch%10 == 0:\n",
    "                print(f\"Epoch {epoch}\\nLoss {mean_loss_batch}\")\n",
    "                print(f\"stdev: {stop[1]}\\n\")\n",
    "            epoch+=1\n",
    "    \n",
    "        # perform test\n",
    "        with torch.no_grad():\n",
    "            validation_batch_losses = []\n",
    "            for features, labels in test_set_loader:\n",
    "                outputs = co2_rnn_model(features)\n",
    "        \n",
    "                loss = criterion(outputs,labels)\n",
    "                validation_batch_losses.append(loss.item())\n",
    "        \n",
    "            validation_batch_loss_mean = statistics.mean(validation_batch_losses)\n",
    "            print(f\"Test Loss: {validation_batch_loss_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc18bb2",
   "metadata": {},
   "source": [
    "# Anomaly temperatures prediction model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        dataset = anomaly_temperatures_dataset\n",
    "        batch_size = 10\n",
    "        train_size = 0.8\n",
    "        train_size = int(train_size * len(dataset))\n",
    "        train_set, test_set = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "        train_set_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        test_set_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "        temp_anomaly_lstm_model = LSTM(1, 40, 1, 2).to(device)\n",
    "        \n",
    "        # loss and optimizer selection\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(temp_anomaly_lstm_model.parameters(), lr=0.001)\n",
    "        \n",
    "        # train until convergence\n",
    "        losses = []\n",
    "        history_loss = []\n",
    "        epoch = 1\n",
    "        while True:\n",
    "        \n",
    "            loss_batch = []\n",
    "            for i, (features, labels) in enumerate(train_set_loader):\n",
    "        \n",
    "                # Forward pass\n",
    "                outputs = temp_anomaly_lstm_model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    " \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_batch.append(loss.item())\n",
    "                \n",
    "            # check stopping criterion every epoch\n",
    "            mean_loss_batch = statistics.mean(loss_batch)\n",
    "            losses.append(mean_loss_batch)\n",
    "            history_loss.append(mean_loss_batch)\n",
    "            stop = stop_criteria(losses, 0.0005)\n",
    "            if stop[0] or epoch==5000:\n",
    "                break\n",
    "            if len(losses) == 10:\n",
    "                losses = losses[1:]\n",
    "            if epoch%10 == 0:\n",
    "                print(f\"Epoch {epoch}\\nLoss {mean_loss_batch}\")\n",
    "                print(f\"stdev: {stop[1]}\\n\")\n",
    "            epoch+=1\n",
    "    \n",
    "        # perform validation\n",
    "        with torch.no_grad():\n",
    "            validation_batch_losses = []\n",
    "            for features, labels in test_set_loader:\n",
    "                outputs = temp_anomaly_lstm_model(features)\n",
    "        \n",
    "                loss = criterion(outputs,labels)\n",
    "                validation_batch_losses.append(loss.item())\n",
    "        \n",
    "            validation_batch_loss_mean = statistics.mean(validation_batch_losses)\n",
    "            print(f\"Test Loss: {validation_batch_loss_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2392ddbc",
   "metadata": {},
   "source": [
    "# Sea level future predictions model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff80dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "        dataset = sea_level_dataset\n",
    "        batch_size = 10\n",
    "        train_size = 0.8\n",
    "        train_size = int(train_size * len(dataset))\n",
    "        train_set, test_set = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "        train_set_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        test_set_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "        sea_level_lstm_model = LSTM(3, 40, 1, 1).to(device)\n",
    "        \n",
    "        # loss and optimizer selection\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(sea_level_lstm_model.parameters(), lr=0.001)\n",
    "        \n",
    "        # train until convergence\n",
    "        losses = []\n",
    "        history_loss = []\n",
    "        epoch = 1\n",
    "        while True:\n",
    "        \n",
    "            loss_batch = []\n",
    "            for i, (features, labels) in enumerate(train_set_loader):\n",
    "        \n",
    "                # Forward pass\n",
    "                outputs = sea_level_lstm_model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    " \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_batch.append(loss.item())\n",
    "                \n",
    "            # check stopping criterion every epoch\n",
    "            mean_loss_batch = statistics.mean(loss_batch)\n",
    "            losses.append(mean_loss_batch)\n",
    "            history_loss.append(mean_loss_batch)\n",
    "            stop = stop_criteria(losses, 0.0005)\n",
    "            if stop[0] or epoch==50000:\n",
    "                break\n",
    "            if len(losses) == 10:\n",
    "                losses = losses[1:]\n",
    "            if epoch%10 == 0:\n",
    "                print(f\"Epoch {epoch}\\nLoss {mean_loss_batch}\")\n",
    "                print(f\"stdev: {stop[1]}\\n\")\n",
    "            epoch+=1\n",
    "    \n",
    "        # perform validation\n",
    "        with torch.no_grad():\n",
    "            validation_batch_losses = []\n",
    "            for features, labels in test_set_loader:\n",
    "                outputs = temp_anomaly_lstm_model(features)\n",
    "        \n",
    "                loss = criterion(outputs,labels)\n",
    "                validation_batch_losses.append(loss.item())\n",
    "        \n",
    "            validation_batch_loss_mean = statistics.mean(validation_batch_losses)\n",
    "            print(f\"Test Loss: {validation_batch_loss_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e58a945",
   "metadata": {},
   "source": [
    "# Future predictions with the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3719766",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 20\n",
    "time_series = co2_dataset[-1][0].unsqueeze(0).detach().clone()\n",
    "input_sea_level = sea_level_dataset[-1][0][-1:,-3:].unsqueeze(0)\n",
    "co2_future_values = []\n",
    "temp_future_land = []\n",
    "temp_future_ocean = []\n",
    "sea_level_future = []\n",
    "\n",
    "\n",
    "for x in range(year):\n",
    "    temp_prediction_co2 = co2_rnn_model(time_series)\n",
    "    co2_future_values.append(float(temp_prediction_co2[0][0]))\n",
    "    time_series = torch.cat((time_series, temp_prediction_co2.unsqueeze(0)),1)\n",
    "    time_series = time_series[:,1:,:]\n",
    "    \n",
    "    # prediction temperature anomalies\n",
    "    temp_prediction_anomaly = temp_anomaly_lstm_model(time_series[:,:10,:])\n",
    "    time_series = time_series[:,1:,:]\n",
    "    temp_future_land.append(float(temp_prediction_anomaly[0][0]))\n",
    "    temp_future_ocean.append(float(temp_prediction_anomaly[0][1]))\n",
    "    temp_prediction_sea_level = sea_level_lstm_model(input_sea_level)\n",
    "    sea_level_future.append(float(temp_prediction_sea_level[0][0]))\n",
    "    v = torch.cat((temp_prediction_anomaly, temp_prediction_sea_level), 1)\n",
    "    input_sea_level = torch.cat((input_sea_level, v.unsqueeze(0)), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
